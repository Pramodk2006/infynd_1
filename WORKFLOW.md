# Visual Workflow Guide

## ğŸ”„ Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INPUT SOURCES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“„ PDF Files    ğŸŒ URLs    ğŸ“ HTML Files    ğŸ“‹ Text Files      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXTRACTOR FACTORY                             â”‚
â”‚              (Auto-detect source type)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€â”€â”€â”€â–º PDFExtractor (PyMuPDF)
             â”œâ”€â”€â”€â”€â–º HTMLExtractor (BeautifulSoup4)
             â”œâ”€â”€â”€â”€â–º URLExtractor (httpx + BS4)
             â””â”€â”€â”€â”€â–º TextExtractor
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTENT EXTRACTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Parse document structure                                      â”‚
â”‚  â€¢ Extract text content                                          â”‚
â”‚  â€¢ Extract metadata (title, author, date)                        â”‚
â”‚  â€¢ Extract structured elements (headings, lists, tables)         â”‚
â”‚  â€¢ Clean and normalize text                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TEXT PROCESSING                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Clean whitespace and formatting                               â”‚
â”‚  â€¢ Remove unwanted elements (scripts, styles)                    â”‚
â”‚  â€¢ Chunk text (512 chars, 50 overlap)                           â”‚
â”‚  â€¢ Break at sentence boundaries                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT MODEL                                â”‚
â”‚                   (Pydantic Validation)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Document {                                                      â”‚
â”‚    document_id: UUID                                             â”‚
â”‚    source: {type, uri, company, timestamp}                       â”‚
â”‚    metadata: {title, description, page_count}                    â”‚
â”‚    content: {                                                    â”‚
â”‚      raw_text: str                                               â”‚
â”‚      chunks: [ContentChunk]                                      â”‚
â”‚      structured: StructuredContent                               â”‚
â”‚    }                                                             â”‚
â”‚  }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT STORE                                â”‚
â”‚                   (JSON File Storage)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  data/outputs/{company}/                                         â”‚
â”‚    â”œâ”€â”€ metadata.json                                             â”‚
â”‚    â”œâ”€â”€ index.json                                                â”‚
â”‚    â””â”€â”€ sources/                                                  â”‚
â”‚        â””â”€â”€ {timestamp}_{type}_{id}.json                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Web Crawling Modes

### Summary Mode (Default)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SUMMARY MODE                               â”‚
â”‚              (Fast, 2 pages maximum)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Fetch Homepage
    â†“
    https://company.com
    â†“
    Extract content
    â†“
Step 2: Find key page (about/products/services)
    â†“
    https://company.com/about
    â†“
    Extract content
    â†“
Step 3: Combine & return

Result: Quick company overview from 2 pages
Time: ~3 seconds (1s delay between requests)
```

### Full Mode

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FULL MODE                                â”‚
â”‚         (Comprehensive, up to max_pages)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Start at homepage
    â†“
    https://company.com
    â†“
Step 2: Extract all internal links
    â†“
    Queue: [/about, /products, /team, /contact, ...]
    â†“
Step 3: Visit each page (BFS)
    â”œâ”€â–º Check robots.txt
    â”œâ”€â–º Skip if visited
    â”œâ”€â–º Extract content
    â”œâ”€â–º Find new links
    â””â”€â–º Add to queue
    â†“
Step 4: Continue until max_pages or queue empty
    â†“
Step 5: Combine all content & return

Result: Complete website content
Time: ~50+ seconds for 50 pages (1s delay each)
```

## ğŸ“Š Data Model Structure

```
Document
â”œâ”€â”€ document_id: str (UUID)
â”‚
â”œâ”€â”€ source: Source
â”‚   â”œâ”€â”€ type: "pdf" | "url" | "html" | "text"
â”‚   â”œâ”€â”€ uri: str (path or URL)
â”‚   â”œâ”€â”€ company: str
â”‚   â””â”€â”€ extracted_at: datetime
â”‚
â”œâ”€â”€ metadata: Metadata
â”‚   â”œâ”€â”€ title: Optional[str]
â”‚   â”œâ”€â”€ author: Optional[str]
â”‚   â”œâ”€â”€ description: Optional[str]
â”‚   â”œâ”€â”€ page_count: Optional[int]
â”‚   â”œâ”€â”€ language: Optional[str]
â”‚   â””â”€â”€ extra: Dict[str, Any]
â”‚
â””â”€â”€ content: Content
    â”œâ”€â”€ raw_text: str (full text)
    â”‚
    â”œâ”€â”€ chunks: List[ContentChunk]
    â”‚   â””â”€â”€ ContentChunk
    â”‚       â”œâ”€â”€ chunk_id: str (UUID)
    â”‚       â”œâ”€â”€ text: str
    â”‚       â”œâ”€â”€ start_index: int
    â”‚       â”œâ”€â”€ end_index: int
    â”‚       â””â”€â”€ metadata: Dict[str, Any]
    â”‚
    â””â”€â”€ structured: Optional[StructuredContent]
        â”œâ”€â”€ headings: List[Dict]
        â”‚   â””â”€â”€ {tag: "h1", text: "..."}
        â”œâ”€â”€ paragraphs: List[str]
        â”œâ”€â”€ lists: List[Dict]
        â”‚   â””â”€â”€ {type: "ul", items: [...]}
        â”œâ”€â”€ tables: List[Dict]
        â”‚   â””â”€â”€ {headers: [...], rows: [[...]]}
        â””â”€â”€ links: List[Dict]
            â””â”€â”€ {text: "...", href: "..."}
```

## ğŸ”§ CLI Command Flow

### Extract Command

```
$ python main.py extract "https://example.com" "Example Corp" --crawl-mode summary

    â†“
[1] Parse arguments
    â”œâ”€â”€ source: "https://example.com"
    â”œâ”€â”€ company: "Example Corp"
    â””â”€â”€ crawl_mode: "summary"
    â†“
[2] Detect source type
    â””â”€â”€ Type: url
    â†“
[3] Get extractor from factory
    â””â”€â”€ URLExtractor
    â†“
[4] Extract content (with progress spinner)
    â”œâ”€â”€ Fetch homepage
    â”œâ”€â”€ Find key pages
    â”œâ”€â”€ Extract from 2 pages
    â””â”€â”€ Build Document
    â†“
[5] Save to storage
    â””â”€â”€ data/outputs/example-corp/sources/{timestamp}_url_{id}.json
    â†“
[6] Display summary
    â”œâ”€â”€ Document ID
    â”œâ”€â”€ Title
    â”œâ”€â”€ Text length
    â”œâ”€â”€ Number of chunks
    â””â”€â”€ Save path
```

### Batch Command

```
$ python main.py batch "Example Corp" file1.pdf https://example.com file2.html

    â†“
[1] Parse arguments
    â”œâ”€â”€ company: "Example Corp"
    â””â”€â”€ sources: [file1.pdf, https://example.com, file2.html]
    â†“
[2] Loop through sources
    â”œâ”€â”€ Source 1: file1.pdf
    â”‚   â”œâ”€â”€ Detect type: pdf
    â”‚   â”œâ”€â”€ Extract with PDFExtractor
    â”‚   â””â”€â”€ Save
    â”‚
    â”œâ”€â”€ Source 2: https://example.com
    â”‚   â”œâ”€â”€ Detect type: url
    â”‚   â”œâ”€â”€ Extract with URLExtractor
    â”‚   â””â”€â”€ Save
    â”‚
    â””â”€â”€ Source 3: file2.html
        â”œâ”€â”€ Detect type: html
        â”œâ”€â”€ Extract with HTMLExtractor
        â””â”€â”€ Save
    â†“
[3] Display summary
    â”œâ”€â”€ Total sources: 3
    â”œâ”€â”€ Successful: 3
    â””â”€â”€ Failed: 0
```

## ğŸ“‚ Output File Structure

```
data/outputs/
â”‚
â”œâ”€â”€ example-corp/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â”‚   {
â”‚   â”‚     "company": "Example Corp",
â”‚   â”‚     "created": "2025-12-17T10:00:00",
â”‚   â”‚     "last_updated": "2025-12-17T12:30:00",
â”‚   â”‚     "total_sources": 5
â”‚   â”‚   }
â”‚   â”‚
â”‚   â”œâ”€â”€ index.json
â”‚   â”‚   {
â”‚   â”‚     "sources": [
â”‚   â”‚       {
â”‚   â”‚         "document_id": "abc-123",
â”‚   â”‚         "type": "url",
â”‚   â”‚         "uri": "https://example.com",
â”‚   â”‚         "title": "Example Corp - Homepage",
â”‚   â”‚         "extracted_at": "2025-12-17T10:00:00",
â”‚   â”‚         "filepath": "sources/20251217_100000_url_abc123.json"
â”‚   â”‚       },
â”‚   â”‚       ...
â”‚   â”‚     ]
â”‚   â”‚   }
â”‚   â”‚
â”‚   â””â”€â”€ sources/
â”‚       â”œâ”€â”€ 20251217_100000_url_abc123.json
â”‚       â”œâ”€â”€ 20251217_100530_pdf_def456.json
â”‚       â””â”€â”€ 20251217_101000_html_ghi789.json
â”‚
â”œâ”€â”€ acme-corporation/
â”‚   â”œâ”€â”€ metadata.json
â”‚   â”œâ”€â”€ index.json
â”‚   â””â”€â”€ sources/
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ techstart-inc/
    â”œâ”€â”€ metadata.json
    â”œâ”€â”€ index.json
    â””â”€â”€ sources/
        â””â”€â”€ ...
```

## ğŸ¨ Terminal Output Examples

### Successful Extraction

```
B2B Data Fusion Engine
Company: Acme Corporation
Source: https://acme.com
Type: url
Crawl mode: summary

â ‹ Extracting content...

âœ“ Extraction complete!

Document ID:      abc-123-def-456
Title:            Acme Corporation - Innovative Solutions
Raw text length:  15,234 characters
Chunks:           30
Headings:         12
Paragraphs:       45
Lists:            5
Tables:           2
Saved to:         data/outputs/acme-corporation/sources/20251217_120000_url_abc123.json
```

### Batch Processing

```
B2B Data Fusion Engine - Batch Processing
Company: Acme Corporation
Sources: 3

Processing 1/3: brochure.pdf
  Type: pdf
  â ‹ Extracting...
  âœ“ Saved to 20251217_120000_pdf_abc123.json

Processing 2/3: https://acme.com
  Type: url
  â ‹ Extracting...
  âœ“ Saved to 20251217_120530_url_def456.json

Processing 3/3: about.html
  Type: html
  â ‹ Extracting...
  âœ“ Saved to 20251217_121000_html_ghi789.json

Batch processing complete!
Successful: 3/3
```

### Company Info

```
Company Information
Name:          Acme Corporation
Total sources: 5
Created:       2025-12-17 10:00:00
Last updated:  2025-12-17 12:30:00

Sources:

â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Type â”‚ Title                   â”‚ URI                  â”‚ Extracted           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ url  â”‚ Acme Corp - Homepage    â”‚ https://acme.com     â”‚ 2025-12-17 10:00:00 â”‚
â”‚ pdf  â”‚ Annual Report 2024      â”‚ /path/to/report.pdf  â”‚ 2025-12-17 10:05:00 â”‚
â”‚ html â”‚ About Acme              â”‚ /path/to/about.html  â”‚ 2025-12-17 10:10:00 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start Workflow

```
Step 1: Install
â”œâ”€ pip install -r requirements.txt

Step 2: Verify
â”œâ”€ python verify_setup.py

Step 3: Create test data
â”œâ”€ python create_test_data.py

Step 4: Test extraction
â”œâ”€ python main.py extract test_data/acme_about.html "Acme Corp"

Step 5: View results
â”œâ”€ python main.py list-companies
â””â”€ python main.py info "Acme Corp"

Step 6: Use with real data
â”œâ”€ python main.py extract "https://realcompany.com" "Real Company"
â””â”€ python main.py batch "Real Company" doc1.pdf doc2.html
```

---

**You now have a complete visual guide to the data extraction pipeline!** ğŸ‰
